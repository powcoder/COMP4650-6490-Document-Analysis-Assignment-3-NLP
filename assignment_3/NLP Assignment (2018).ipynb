{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys, getopt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "# Constants\n",
    "label_to_id = {'World':0, 'Entertainment':1, 'Sports':2}\n",
    "num_classes = 3\n",
    "pad_word_id = 0\n",
    "unknown_word_id = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3\n",
    "embedding_dim = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Batch training [3 pts].\n",
    "\n",
    "Batch training is widely used for finding optimal parameters of neural network models. In Tensorflow, a batch corresponds to an input tensor of fixed size. However, it imposes a special challenge when input data is word sequences of variable length. A widely used technique is to assume that there is a maximal sequence length so that all sequences in a batch are fitted into tensors of the same dimensions. For sequences shorter than the maximal length, we append them with pad words so that all sequences in a batch are of the same length. A pad word is a special token, whose embedding is an all-zero vector. Your task is to implement the padding technique and make sure the pad words do not change model outputs as well as model parameters during training. You should implement the padding technique by completing the fasttext model [1] below to support batch training. Note that, you need to make sure that i) all elements of the pad word embedding are zero during training; ii) your implementation should pass the unit tests in FastTextTest; iii) your implementation should be able to work on the provided news title classification dataset.\n",
    "\n",
    "Hints:\n",
    "  -  You may modify the unit tests accordingly but should not change the goals of the existing tests.\n",
    "  -  Practice your Tensorflow skills in the provided warm-up exercise though the exercise will not be graded.\n",
    "\n",
    "[1] Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759., 2016.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(object):\n",
    "    \"\"\"Define the computation graph for fasttext model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, embedding_dim, size_vocab, learning_rate, batch_size):\n",
    "        \"\"\"Init the model with default parameters/hyperparameters.\"\"\"\n",
    "        assert(size_vocab > 2)\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.size_vocab = size_vocab\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"Build the computation graph.\"\"\"\n",
    "        self.declare_placeholders()\n",
    "        self.declare_variables()\n",
    "        self.inference()\n",
    "        self.optimize()\n",
    "        self.predict()\n",
    "        self.compute_accuracy()\n",
    "        \n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Declare all place holders.\"\"\"\n",
    "        with tf.name_scope('fast_text'):\n",
    "            self.input_sens = tf.placeholder(tf.int32, shape = [None, None], name = 'input_sens')\n",
    "            self.sens_length = tf.placeholder(tf.float32, shape = [None, 1], name = 'sens_length')\n",
    "            self.correct_label = tf.placeholder(tf.float32, shape=[None, self.num_classes], name = 'correct_label')\n",
    "        \n",
    "    def declare_variables(self):\n",
    "        \"\"\"Declare all variables.\"\"\"\n",
    "        with tf.name_scope('fast_text'):\n",
    "            self.W = tf.Variable(tf.zeros([self.embedding_dim, self.num_classes]), name = 'W')\n",
    "            self.b = tf.Variable(tf.zeros([1, self.num_classes]), name = 'b')\n",
    "            # Hint: Initialize word embeddings properly.\n",
    "            embed_matrix = np.random.uniform(-1,1, [self.size_vocab, self.embedding_dim])\n",
    "            self.embeddings = tf.Variable(embed_matrix, dtype=tf.float32, name = 'embed')\n",
    "    \n",
    "    def compute_mean_vector(self):\n",
    "        # Hints:\n",
    "        # Make sure that the embedding of the pad word is not updated during BackProp.\n",
    "        # Make sure that the mean of each instance is correctly computed.\n",
    "        embed_seq = tf.nn.embedding_lookup(self.embeddings, self.input_sens)\n",
    "        # Compute the means here.\n",
    "        # Hint:\n",
    "        # There are more than one way of doing this.\n",
    "        # https://www.tensorflow.org/performance/xla/broadcasting\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/tile\n",
    "        \n",
    "       \n",
    "    \n",
    "    def inference(self):\n",
    "        \"\"\"Compute the logits of x.\"\"\"\n",
    "        self.compute_mean_vector()\n",
    "        self.logit = tf.matmul(self.mean_rep, self.W) + self.b\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"Train a fast text model from scratch.\"\"\"\n",
    "        self.loss()\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate, name = 'SGD')\n",
    "        self.train_step = optimizer.minimize(self.cross_entropy, name = 'train_step')\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        \"\"\"Evaluate the model against a test/validation set\"\"\"\n",
    "        correct_prediction = tf.equal(self.prediction, tf.argmax(self.correct_label, 1))\n",
    "        self.accuracy = tf.cast(correct_prediction, tf.float32, name = 'accuracy')\n",
    "        \n",
    "    def predict(self):\n",
    "        self.prediction = tf.argmax(self.logit, 1)\n",
    "    \n",
    "    def loss(self):\n",
    "        \"\"\"Compute the loss of a batch.\"\"\"\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.correct_label, logits = self.logit))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(size_vocab, max_length, batch_size):\n",
    "    \"\"\" Generate input data to simulate sentences.\n",
    "    \n",
    "    Args:\n",
    "        size_vocab (int) : The size of the vocabulary.\n",
    "        max_length (int) : The maximal sequence length.\n",
    "        batch_size (int) : Batch size.\n",
    "    \"\"\"\n",
    "    input_sens = np.random.randint(1, size_vocab, size = [batch_size, max_length])\n",
    "    sens_length = np.zeros(shape=[batch_size, 1], dtype=np.float32, order='C')\n",
    "    # Hints: You may add additional inputs here.\n",
    "    if max_length > 1:\n",
    "        for i in range(0, batch_size):\n",
    "            sens_m_length = random.randint(1, max_length)\n",
    "            sens_length[i] = sens_m_length\n",
    "            for j in range(sens_m_length, max_length):\n",
    "                input_sens[i, j] = 0\n",
    "            \n",
    "    return (input_sens,sens_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(batch_size, num_classes):\n",
    "    \"\"\" Generate input data to simulate sentences.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int) : number of classes.\n",
    "        batch_size (int) : batch size.\n",
    "    \"\"\"\n",
    "    true_labels = np.random.randint(0, num_classes, size = batch_size)\n",
    "    label_matrix = np.zeros(shape=[batch_size, num_classes], dtype=np.intp)\n",
    "    for i in range(0, batch_size):\n",
    "        label_matrix[i, true_labels[i]] = 1\n",
    "    return label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(size_vocab, embedding_dim):\n",
    "    \"\"\" Initialize word embedding matrix.\n",
    "    \n",
    "    Args:\n",
    "        size_vocab (int) : size of the vocabulary.\n",
    "        batch_size (int) : batch size.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.random.uniform(-1, 1, [size_vocab, embedding_dim])\n",
    "    # How to deal with the pad word?\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTest(tf.test.TestCase):\n",
    "    \"\"\"https://guillaumegenthial.github.io/testing.html\"\"\"\n",
    "    \n",
    "    def test_computing_mean(self):\n",
    "        \"\"\"\n",
    "            The means computed by Numpy should match the ones computed by FastText.\n",
    "        \"\"\"\n",
    "        vocab_size = 10\n",
    "        embed_dim = 3\n",
    "        num_inst_batch = 2\n",
    "        sens, sens_length = generate_inputs(vocab_size, 5, num_inst_batch)\n",
    "        embed_matrix = init_embeddings(vocab_size, embed_dim)\n",
    "        sens_embedding = np.take(embed_matrix, sens, 0)\n",
    "        true_mean = np.sum(sens_embedding,1) / sens_length\n",
    "        \n",
    "        model = FastText(num_classes = 3, embedding_dim = embed_dim, size_vocab = vocab_size, learning_rate = 0.1, batch_size = num_inst_batch)\n",
    "        model.input_sens = tf.placeholder(tf.int32, shape = [None, None], name = 'input_sens')\n",
    "        model.sens_length = tf.placeholder(tf.float32, shape = [None, None], name = 'sens_length')\n",
    "        model.embeddings = tf.Variable(embed_matrix, name = 'embed', dtype=tf.float32)\n",
    "        model.compute_mean_vector()\n",
    "        with self.test_session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            mean_vec = sess.run(model.mean_rep, \n",
    "                                             feed_dict={model.input_sens: sens, model.sens_length : sens_length})\n",
    "            self.assertAllClose(mean_vec, true_mean,rtol=1e-06, atol=1e-06, msg=\"Mean vectors are not equal.\")\n",
    "    \n",
    "    def test_loss(self):\n",
    "        \"\"\"\n",
    "            The loss computed by Numpy should be the same as the one from the model.\n",
    "        \"\"\"\n",
    "        vocab_size = 10\n",
    "        embed_dim = 3\n",
    "        num_inst_batch = 2\n",
    "        num_classes = 3\n",
    "        sens, sens_length = generate_inputs(vocab_size, 5, num_inst_batch)\n",
    "        true_labels = generate_labels(num_inst_batch, num_classes)\n",
    "        \n",
    "        embed_matrix = init_embeddings(vocab_size, embed_dim)\n",
    "        sens_embedding = np.take(embed_matrix, sens, 0)\n",
    "        true_mean = np.sum(sens_embedding,1) / sens_length\n",
    "        W = np.random.uniform(-1, 1, size = [embed_dim, num_classes])\n",
    "        b = np.random.uniform(-1, 1, size = [1, num_classes])\n",
    "        true_logit = np.matmul(true_mean, W) + b\n",
    "        exp_logit = np.exp(true_logit)\n",
    "        true_denominator = np.log(np.sum(exp_logit, 1))\n",
    "        true_scores = np.sum(np.multiply(true_logit, true_labels),1)\n",
    "        true_loss = np.mean(true_denominator - true_scores)\n",
    "        \n",
    "        model = FastText(num_classes, embedding_dim = embed_dim, size_vocab = vocab_size, learning_rate = 0.1, batch_size = num_inst_batch)\n",
    "        model.input_sens = tf.placeholder(tf.int32, shape = [num_inst_batch, None], name = 'input_sens')\n",
    "        model.sens_length = tf.placeholder(tf.float32, shape = [num_inst_batch, None], name = 'sens_length')\n",
    "        model.correct_label = tf.placeholder(tf.float32, shape = [num_inst_batch, num_classes], name = 'correct_labels')\n",
    "        \n",
    "        model.embeddings = tf.Variable(embed_matrix, name = 'embed', dtype=tf.float32)\n",
    "        model.W = tf.Variable(W, name = 'W', dtype=tf.float32)\n",
    "        model.b = tf.Variable(b, name = 'b', dtype=tf.float32)\n",
    "        model.inference()\n",
    "        model.loss()\n",
    "        with self.test_session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            loss = sess.run(model.cross_entropy, \n",
    "                                             feed_dict={model.input_sens: sens, model.sens_length : sens_length, model.correct_label : true_labels})\n",
    "            self.assertAllClose(loss, true_loss,rtol=1e-06, atol=1e-06, msg=\"cross entropy is not equal.\")\n",
    "\n",
    "    def test_computing_accuracy(self):\n",
    "        \"\"\" The accuracy computed by Numpy should match the one computed by the model.\n",
    "        \"\"\"\n",
    "        vocab_size = 10\n",
    "        embed_dim = 4\n",
    "        num_inst_batch = 2\n",
    "        num_classes = 3\n",
    "        sens, sens_length = generate_inputs(vocab_size, 5, num_inst_batch)\n",
    "        true_labels = generate_labels(num_inst_batch, num_classes)\n",
    "        \n",
    "        embed_matrix = init_embeddings(vocab_size, embed_dim)\n",
    "        sens_embedding = np.take(embed_matrix, sens, 0)\n",
    "        true_mean = np.sum(sens_embedding,1) / sens_length\n",
    "        W = np.random.uniform(-1, 1, size = [embed_dim, num_classes])\n",
    "        b = np.random.uniform(-1, 1, size = [1, num_classes])\n",
    "        true_logit = np.matmul(true_mean, W) + b\n",
    "        true_prediction = np.argmax(true_logit, 1)\n",
    "        true_label_indices = np.argmax(true_labels, 1)\n",
    "        boolean_accuracy_matrix = np.equal(true_prediction, true_label_indices)\n",
    "        true_accuracy_matrix = boolean_accuracy_matrix.astype(np.float32)\n",
    "        \n",
    "        model = FastText(num_classes, embedding_dim = embed_dim, size_vocab = vocab_size, learning_rate = 0.1, batch_size = num_inst_batch)\n",
    "        model.input_sens = tf.placeholder(tf.int32, shape = [num_inst_batch, None], name = 'input_sens')\n",
    "        model.sens_length = tf.placeholder(tf.float32, shape = [num_inst_batch, None], name = 'sens_length')\n",
    "        model.correct_label = tf.placeholder(tf.float32, shape = [num_inst_batch, num_classes], name = 'correct_labels')\n",
    "        \n",
    "        model.embeddings = tf.Variable(embed_matrix, name = 'embed', dtype=tf.float32)\n",
    "        model.W = tf.Variable(W, name = 'W', dtype=tf.float32)\n",
    "        model.b = tf.Variable(b, name = 'b', dtype=tf.float32)\n",
    "        model.inference()\n",
    "        model.predict()\n",
    "        model.compute_accuracy()\n",
    "        with self.test_session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            accuracy = sess.run(model.accuracy, \n",
    "                                             feed_dict={model.input_sens: sens, model.sens_length : sens_length, model.correct_label : true_labels})\n",
    "            self.assertAllEqual(accuracy, true_accuracy_matrix, msg=\"accuracy is not equal.\")\n",
    "            \n",
    "    def test_zero_embeddings(self):\n",
    "        \"\"\" The embedding of the pad word should be all zeros after a few training epochs.\n",
    "        \"\"\"\n",
    "        vocab_size = 10\n",
    "        embed_dim = 4\n",
    "        num_inst_batch = 2\n",
    "        num_classes = 3\n",
    "        model = FastText(num_classes, embedding_dim = embed_dim, size_vocab = vocab_size, learning_rate = 0.1, batch_size = num_inst_batch)\n",
    "        model.build_graph()\n",
    "        pad_word_embeddings = np.zeros(embed_dim)\n",
    "        with self.test_session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(0, 6):\n",
    "                sens, sens_length = generate_inputs(vocab_size, 5, num_inst_batch)\n",
    "                true_labels = generate_labels(num_inst_batch, num_classes)    \n",
    "                train_step, loss, embeddings = sess.run([model.train_step, model.cross_entropy, model.embeddings], \n",
    "                                                 feed_dict={model.input_sens: sens, model.sens_length : sens_length, model.correct_label : true_labels})\n",
    "                self.assertAllEqual(embeddings[pad_word_id, :], pad_word_embeddings, msg = \"Epoch {} : the embedding of the pad word contains non-zeros.\".format(epoch))\n",
    "    \n",
    "    def runTest(self):\n",
    "        pass\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    test=FastTextTest()\n",
    "    test.test_computing_mean()\n",
    "    test.test_loss()\n",
    "    test.test_computing_accuracy()\n",
    "    test.test_zero_embeddings()\n",
    "    print(\"All tests are passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "        The class for representing a dataset.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, labels, max_length = -1):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                sentences (list) : a list of sentences. Each sentence is a list of word ids.\n",
    "                labels (list) : a list of label representations. Each label is represented by one-hot vector.\n",
    "        \"\"\"\n",
    "        if len(sentences) != len(labels):\n",
    "            raise ValueError(\"The size of sentences {} does not match the size of labels {}. \".format(len(sentences), len(labels)) )\n",
    "        if len(labels) == 0:\n",
    "            raise ValueError(\"The input is empty.\")\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.sens_lengths = [len(sens) for sens in sentences]\n",
    "        if max_length == -1:\n",
    "            self.max_sens_length = max(self.sens_lengths)\n",
    "        else:\n",
    "            self.max_sens_length = max_length\n",
    "        \n",
    "        \n",
    "    def label_tensor(self):\n",
    "        \"\"\"\n",
    "            Return the label matrix of a batch.\n",
    "        \"\"\"\n",
    "        return np.array(self.labels)\n",
    "    \n",
    "    def sent_tensor(self):\n",
    "        \"\"\"\n",
    "            Return the sentence matrix of a batch.\n",
    "        \"\"\"\n",
    "        return np.array(self.sentences)\n",
    "    \n",
    "    def sens_length(self):\n",
    "        \"\"\"\n",
    "            Return a vector of sentence length for a batch.\n",
    "        \"\"\"\n",
    "        length_array = np.array(self.sens_lengths, dtype=np.float32)\n",
    "        return np.reshape(length_array, [len(self.sens_lengths),1])\n",
    "   \n",
    "    def subset(self, index_list):\n",
    "        \"\"\" Return a subset of the dataset.\n",
    "        \n",
    "            Args:\n",
    "                index_list (list) : a list of sentence index.\n",
    "        \"\"\"\n",
    "        sens_subset = []\n",
    "        labels_subset = []\n",
    "        for index in index_list:\n",
    "            if index >= len(self.sentences):\n",
    "                raise IndexError(\"index {} is larger than or equal to the size of the dataset {}.\".format(index, len(self.sentences)))\n",
    "            sens_subset.append(self.sentences[index]) \n",
    "            labels_subset.append(self.labels[index])\n",
    "            \n",
    "        dataset = Dataset(sentences=sens_subset, labels=labels_subset, max_length=self.max_sens_length)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_batch(self, index_list):\n",
    "        \"\"\" Return a batch.\n",
    "            \n",
    "            Args:\n",
    "                index_list (list) : a list of sentence index.\n",
    "        \"\"\"\n",
    "        data_subset = self.subset(index_list)\n",
    "        for sens in data_subset.sentences:\n",
    "            self.pad_sentence(sens)\n",
    "\n",
    "        return data_subset\n",
    "    \n",
    "    def pad_sentence(self, sens):\n",
    "        \"\"\" Implement padding here.\n",
    "            \n",
    "            Args:\n",
    "                sens (list) : a list of word ids.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_vec(label):\n",
    "    \"\"\"Create one hot representation for the given label.\n",
    "    \n",
    "    Args:\n",
    "        label(str): class name\n",
    "    \"\"\"\n",
    "    label_id = label_to_id[label.strip()]\n",
    "    label_vec = np.zeros(num_classes, dtype=np.int)\n",
    "    label_vec[label_id] = 1\n",
    "    return label_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sens):\n",
    "    \"\"\"Tokenize a sentence\n",
    "    \n",
    "    Args:\n",
    "        sens (str) : a sentence.\n",
    "    \"\"\"\n",
    "    return word_tokenize(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_seq_to_word_id_seq(token_seq, word_to_id):\n",
    "    \"\"\" Map a word sequence to a word ID sequence.\n",
    "             \n",
    "    Args:\n",
    "        token_seq (list) : a list of words, each word is a string.\n",
    "        word_to_id (dictionary) : map word to its id.\n",
    "    \"\"\"\n",
    "    return [map_word_to_id(word_to_id,word) for word in token_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_word_to_id(word_to_id, word):\n",
    "    \"\"\" Map a word to its id.\n",
    "    \n",
    "        Args:\n",
    "            word_to_id (dictionary) : a dictionary mapping words to their ids.\n",
    "            word (string) : a word.\n",
    "    \"\"\"\n",
    "    if word in word_to_id:\n",
    "        return word_to_id[word]\n",
    "    else:\n",
    "        return unknown_word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sens_file_name):\n",
    "    \"\"\" Build a vocabulary from a train set.\n",
    "        \n",
    "        Args:\n",
    "            sens_file_name (string) : the file path of the training sentences.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(sens_file_name) as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = tokenize(line)\n",
    "            data.extend(tokens)\n",
    "    print('number of sequences is %s. ' % len(data))\n",
    "    count = [['$PAD$', pad_word_id], ['$UNK$', unknown_word_id]]\n",
    "    sorted_counts = collections.Counter(data).most_common()\n",
    "    count.extend(sorted_counts)\n",
    "    word_to_id = dict()\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    \n",
    "    print(\"PAD word id is %s .\" % word_to_id['$PAD$'])\n",
    "    print(\"Unknown word id is %s .\" % word_to_id['$UNK$'])\n",
    "    print('size of vocabulary is %s. ' % len(word_to_id))\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_dataset(sens_file_name, label_file_name, word_to_id):\n",
    "    \"\"\" Read labeled dataset.\n",
    "    \n",
    "        Args:\n",
    "            sens_file_name (string) : the file path of sentences.\n",
    "            label_file_name (string) : the file path of sentence labels.\n",
    "            word_to_id (dictionary) : a dictionary mapping words to their ids.\n",
    "    \"\"\"\n",
    "    with open(sens_file_name) as sens_file, open(label_file_name) as label_file:\n",
    "        data = []\n",
    "        data_labels = []\n",
    "        for label in label_file:\n",
    "            sens = sens_file.readline()\n",
    "            word_id_seq = map_token_seq_to_word_id_seq(tokenize(sens), word_to_id)\n",
    "            if len(word_id_seq) > 0 :\n",
    "                data.append(word_id_seq)\n",
    "                data_labels.append(create_label_vec(label))\n",
    "        print(\"read %d sentences from %s .\" % (len(data), sens_file_name))\n",
    "        labeled_set = Dataset(sentences=data, labels=data_labels)\n",
    "        \n",
    "        return labeled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(test_results, result_file):\n",
    "    \"\"\" Write predicted labels into file.\n",
    "        \n",
    "        Args:\n",
    "            test_results (list) : a list of predictions.\n",
    "            result_file (string) : the file path of the prediction result.\n",
    "    \"\"\"\n",
    "    with open(result_file, mode='w') as f:\n",
    "         for r in test_results:\n",
    "             f.write(\"%d\\n\" % r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIter:\n",
    "    \"\"\" An iterator of an dataset instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size = 1):\n",
    "        \"\"\" \n",
    "            Args:\n",
    "                dataset (Dataset) : an instance of Dataset.\n",
    "                batch_size (int) : batch size.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.dataset_size = len(dataset.sentences)\n",
    "        self.shuffle_indices = np.arange(self.dataset_size)\n",
    "        self.batch_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches_per_epoch = int(self.dataset_size/float(self.batch_size))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\" return next instance. \"\"\"\n",
    "        \n",
    "        if self.batch_index < self.dataset_size:\n",
    "            i = self.shuffle_indices[self.batch_index]\n",
    "            self.batch_index += 1\n",
    "            return self.dataset.get_batch([i])\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def next_batch(self):\n",
    "        \"\"\" return indices for the next batch. Useful for minibatch learning.\"\"\"\n",
    "        \n",
    "        if self.batch_index < self.num_batches_per_epoch:\n",
    "            start_index = self.batch_index * self.batch_size\n",
    "            end_index = (self.batch_index + 1) * self.batch_size\n",
    "            self.batch_index += 1\n",
    "            return self.dataset.get_batch(self.shuffle_indices[start_index : end_index])\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def has_next(self):\n",
    "        \n",
    "        return self.batch_index < self.num_batches_per_epoch\n",
    "        \n",
    "            \n",
    "    def shuffle(self):\n",
    "        \"\"\" Shuffle the data indices for training\"\"\"\n",
    "        \n",
    "        self.shuffle_indices = np.random.permutation(self.shuffle_indices)\n",
    "        self.batch_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_for_tensorboard(logs_dir = './computation_graphs'):\n",
    "    \"\"\" Save computation graph to logs_dir for tensorboard. \n",
    "    \n",
    "    Args:\n",
    "        logs_dir (string) : file path to serialized computation graphs.\n",
    "    \n",
    "    \"\"\"\n",
    "    fast_text = FastText(num_classes, embedding_dim, 10, learning_rate, 5)\n",
    "    fast_text.build_graph()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        log_writer = tf.summary.FileWriter(logs_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(word_to_id, train_dataset, dev_dataset, test_dataset, model_file_path, batch_size = 1):\n",
    "    \"\"\" Train a fasttext model, evaluate it on the validation set after each epoch, \n",
    "    and choose the best one model to evaluate it on the test set. \n",
    "    \n",
    "    Args:\n",
    "        word_to_id (dictionary) : word to id mapping.\n",
    "        train_dataset (Dataset) : labeled dataset for training.\n",
    "        dev_dataset (Dataset) : labeled dataset for validation.\n",
    "        test_dataset (Dataset) : labeled dataset for test.\n",
    "        model_file_path (string) : model file path.\n",
    "        batch_size (int) : the number of instances in a batch.\n",
    "    \n",
    "    \"\"\"\n",
    "    fast_text = FastText(num_classes, embedding_dim, len(word_to_id),learning_rate, batch_size)\n",
    "    fast_text.build_graph()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    max_accu = 0\n",
    "    max_accu_epoch = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)    \n",
    "        for epoch in range(num_epochs):\n",
    "            dataIterator = DataIter(train_dataset, batch_size)\n",
    "            dataIterator.shuffle()\n",
    "            total_loss = 0\n",
    "            # modify here to use batch training\n",
    "            while dataIterator.has_next():\n",
    "                batch_data = dataIterator.next_batch()\n",
    "                sens = batch_data.sent_tensor()\n",
    "                label = batch_data.label_tensor()\n",
    "                sens_length = batch_data.sens_length()\n",
    "                (optimizer, loss) = sess.run([fast_text.train_step, fast_text.cross_entropy], \n",
    "                                             feed_dict={fast_text.input_sens: sens, fast_text.sens_length : sens_length, fast_text.correct_label: label})\n",
    "                total_loss += np.sum(loss)\n",
    "            model_file = '{}_{}'.format(model_file_path, epoch)\n",
    "            saver.save(sess, model_file)\n",
    "            validation_accuracy = compute_accuracy(fast_text, model_file, dev_dataset)\n",
    "            print('Epoch %d : train loss = %s , validation accuracy = %s .' % (epoch, total_loss, validation_accuracy))\n",
    "            if validation_accuracy > max_accu:\n",
    "                max_accu = validation_accuracy\n",
    "                max_accu_epoch = epoch\n",
    "\n",
    "        # modify here to use batch evaluation\n",
    "    final_model_file = '{}_{}'.format(model_file_path, max_accu_epoch)\n",
    "    print('Accuracy on the test set : %s.' % compute_accuracy(fast_text, final_model_file, test_dataset))\n",
    "    predictions = predict(fast_text, final_model_file, test_dataset)\n",
    "    write_results(predictions, './predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fast_text, fasttext_model_file, test_dataset):\n",
    "    \"\"\" \n",
    "    Predict labels for each sentence in the test_dataset.\n",
    "    \n",
    "    Args:\n",
    "        fast_text (FastText) : an instance of fasttext model.\n",
    "        fasttext_model_file (string) : file path to the fasttext model.\n",
    "        test_dataset (Dataset) : labeled dataset to generate predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    test_results = []\n",
    "    with tf.Session() as sess: \n",
    "        saver.restore(sess, fasttext_model_file)\n",
    "        dataIterator = DataIter(test_dataset)\n",
    "        while dataIterator.has_next():\n",
    "            data_record = dataIterator.next()\n",
    "            sens = data_record.sent_tensor()\n",
    "            sens_length = data_record.sens_length()\n",
    "            prediction = fast_text.prediction.eval(feed_dict={fast_text.input_sens: sens, fast_text.sens_length : sens_length})\n",
    "            test_results.append(prediction)\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(fast_text, fasttext_model_file, eval_dataset):\n",
    "    \"\"\" \n",
    "    Compuate accuracy on the eval_dataset in the batch mode. It is useful only for the bonus assignment.\n",
    "    \n",
    "    Args:\n",
    "        fast_text (FastText) : an instance of fasttext model.\n",
    "        fasttext_model_file (string) : file path to the fasttext model.\n",
    "        eval_dataset (Dataset) : labeled dataset for evaluation.\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "    dataIterator = DataIter(eval_dataset)\n",
    "    \n",
    "    num_correct = 0\n",
    "    with tf.Session() as sess: \n",
    "        saver.restore(sess, fasttext_model_file)\n",
    "        \n",
    "        while dataIterator.has_next():\n",
    "            data_record = dataIterator.next()\n",
    "            sens = data_record.sent_tensor()\n",
    "            label = data_record.label_tensor()\n",
    "            sens_length = data_record.sens_length()\n",
    "            is_correct = sess.run(fast_text.accuracy, \n",
    "                                         feed_dict={fast_text.input_sens: sens, fast_text.sens_length : sens_length, fast_text.correct_label: label})\n",
    "            num_correct += is_correct\n",
    "            \n",
    "    return num_correct/float(eval_dataset.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main(data_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate the fasttext model.\n",
    "    \n",
    "    Args:\n",
    "        data_folder (string) : the path to the data folder.\n",
    "    \n",
    "    \"\"\"\n",
    "    trainSensFile = os.path.join(data_folder, 'sentences_train.txt')\n",
    "    devSensFile = os.path.join(data_folder, 'sentences_dev.txt')\n",
    "    testSensFile = os.path.join(data_folder, 'sentences_test.txt')\n",
    "    trainLabelFile = os.path.join(data_folder, 'labels_train.txt')\n",
    "    devLabelFile = os.path.join(data_folder, 'labels_dev.txt')\n",
    "    testLabelFile = os.path.join(data_folder, 'labels_test.txt')\n",
    "    testResultFile = os.path.join(data_folder, 'test_results.txt')\n",
    "    model_file_path = os.path.join(data_folder, 'fasttext_model_file')\n",
    "\n",
    "    word_to_id = build_vocab(trainSensFile)\n",
    "    train_dataset = read_labeled_dataset(trainSensFile, trainLabelFile, word_to_id)\n",
    "    dev_dataset = read_labeled_dataset(devSensFile, devLabelFile, word_to_id)\n",
    "    test_dataset = read_labeled_dataset(testSensFile, testLabelFile, word_to_id)\n",
    "    eval(word_to_id, train_dataset, dev_dataset, test_dataset, model_file_path, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_main('target_file_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 [3 pts]. Question Classification.\n",
    "\n",
    "Understanding questions is a key problem in chatbots and question answering systems. In the open-domain setting, it is difficult to find right answers in the huge search space. To tackle the problem, one approach is to categorize questions into a finite set of semantic classes, and each semantic class corresponds to a small answer space.\n",
    "\n",
    "Your task is to implement a question classification model in Tensorflow and apply it to the datasets provided in this assignment.\n",
    "\n",
    "Notes: \n",
    "\n",
    "-  The warm-up exercise will not be graded, though following the exercises and instructions may save you a great deal of time. \n",
    "\n",
    "-  Please do not submit your data directories, pretrained word embeddings, and Tensorflow library.\n",
    "\n",
    "-  You may consider reusing part of the code in Q1.\n",
    "\n",
    "-  Code must be submitted with the assignment for purposes of plagiarism detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset provided on Wattle contains three files: **train.json**, **validation.json**, and **test.json**, which are the training dataset, validation dataset, and the test dataset, respectively. \n",
    "See an example below: \n",
    "```\n",
    "{\n",
    "   \"ID\": S1,\n",
    "   \"Label\": 3,\n",
    "   \"Sentence\":\"What country has the best defensive position in the board game Diplomacy ?\"\n",
    "}\n",
    "```\n",
    "In the training set and the validation set, the response variable is called `Label`. Your task is to predict the `Label` for each sentence in the test set. \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The performance of your prediction will be evaluated automatically on Kaggle using __Accuracy__ , which is defined as the number of correct predictions divided by the total number of sentences in the test set (https://classeval.wordpress.com/introduction/basic-evaluation-measures/).\n",
    "\n",
    "Your score will be computed using a lower bound and an upper bound, which will be shown on the Kaggle leader board. \n",
    "Achieving an accuracy equal and below the lower bound amounts to a grade of zero, while achieving the upper bound amounts to the full points (here 3 points, see score distribution here below).\n",
    "Consequently, your score for this competition task will be calculated based on:\n",
    "\n",
    "$$\n",
    "    \\operatorname{Your\\_Score} = \\frac{Your\\_Accuracy - Lower\\_Bound}{Upper\\_Bound - Lower\\_Bound} * 3\n",
    "$$\n",
    "\n",
    "Notes about the lower bound and upper bounds predictors:\n",
    "\n",
    "* The **lower bound** is the performance obtained by a classifer that always picks the majority class according to the class distribution in the training set.\n",
    "* The **upper bound** is generated by an \"in-house\" classifier trained on the same dataset that you were given.\n",
    "\n",
    "There are many possibilities to achieve better results than this. However, the **only** labeled training dataset to train your model should be the provided **train.json**. \n",
    "If you obtain a better performance than the upper bound, then you will have a grade higher than 3 points for this question. This can be useful to compensate for any lost points for the whole assignment.\n",
    "However, the total mark of this assignment is capped at 10 marks.\n",
    "\n",
    "### Kaggle competition\n",
    "\n",
    "- Join the competition [here](https://www.kaggle.com/c/anu-comp4650-assignment-3)\n",
    "- Before submitting the result, first go to **team** menu and change your **team name** as **your university id**.\n",
    "- You need to upload the generated result file to Kaggle. The result file should be in the following format\n",
    "```\n",
    "id,category\n",
    "S101,0\n",
    "S201,1\n",
    "S102,2\n",
    "...\n",
    "```\n",
    "- Note that you are only allowed to upload **5 copies** of your results to Kaggle per day. Make every upload count, and don't waste your opportunities!\n",
    "- For detailed submission instructions, check the end of this notebook.\n",
    "\n",
    "After completion, please rename this notebook to **`your_uid.ipynb` (e.g. `u6000001.ipynb`)**, submit this file to Wattle. Do not upload any other files to Wattle except this notebook file.\n",
    "\n",
    "**Note:** you need to fill in the cells below with your code. If you fail to provide the code, you will get zero for this question. Your code should be well documented and provides methods to generate the prediction files and compute accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 : Comparison between Absolute Discounting and Kneser Ney smoothing. [2pts]\n",
    "\n",
    "Read the code below for interpolated absolute discounting and implement Kneser Ney smoothing in Python. It is sufficient to assume that the highest order of ngram is two and the discount is 0.75. Evaluate your program on the following ngram corpus and compute the distribution $p(x | \\text{Granny})$ for all possible unigrams in the given corpus. Explain what make the differences regarding the prediction results between interpolated absolute discounting and Kneser Ney smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus = ['Sam eats apple',\n",
    "          \"Granny plays with Sam\",\n",
    "           \"Sam plays with Smith\",\n",
    "           \"Sam likes Smith\",\n",
    "          \"Sam likes apple\",\n",
    "                \"Sam likes sport\",\n",
    "                \"Sam plays tennis\",\n",
    "                \"Sam likes games\",\n",
    "                \"Sam plays games\",\n",
    "          \"Sam likes apple Granny Smith\"]\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "class NgramStats:\n",
    "    \"\"\" Collect unigram and bigram statistics. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_to_count = Counter([])\n",
    "        self.unigram_to_count = dict()\n",
    "        \n",
    "    def collect_ngram_counts(self, corpus):\n",
    "        \"\"\"Collect unigram and bigram counts from the given corpus.\"\"\"\n",
    "        unigram_counter = Counter([])\n",
    "        for sentence in corpus:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            bigrams = ngrams(tokens, 2)\n",
    "            unigrams = ngrams(tokens, 1)\n",
    "            self.bigram_to_count += Counter(bigrams)\n",
    "            unigram_counter += Counter(unigrams)\n",
    "        self.unigram_to_count = {k[0]:int(v) for k,v in unigram_counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = NgramStats()         \n",
    "stats.collect_ngram_counts(ngram_corpus)\n",
    "print(stats.bigram_to_count)\n",
    "print(stats.unigram_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated Absolute Discounting\n",
    "import operator\n",
    "    \n",
    "class AbsDist:\n",
    "    \"\"\"\n",
    "     Implementation of Interpolated Absolute Discounting\n",
    "     \n",
    "     Reference: slide 25 in https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_stats):\n",
    "        \"\"\" Initialization\n",
    "        \n",
    "            Args:\n",
    "                ngram_stats (NgramStats) : ngram statistics.\n",
    "        \"\"\"\n",
    "        self.unigram_freq = float(sum(ngram_stats.unigram_to_count.values()))\n",
    "        self.stats= ngram_stats\n",
    "    \n",
    "    def compute_prop(self, bigram, discount = 0.75):\n",
    "        \"\"\" Compute probability p(y | x)\n",
    "        \n",
    "            Args:\n",
    "                bigram (string tuple) : a bigram (x, y), where x and y denotes an unigram respectively.\n",
    "                discount (float) : the discounter factor for the linear interpolation.\n",
    "        \"\"\"\n",
    "        preceding_word_count = 0\n",
    "        if bigram[0] in self.stats.unigram_to_count:\n",
    "            preceding_word_count = self.stats.unigram_to_count[bigram[0]]\n",
    "            \n",
    "        if preceding_word_count > 0:\n",
    "            left_term = 0\n",
    "            if bigram in self.stats.bigram_to_count:\n",
    "                bigram_count = float(self.stats.bigram_to_count[bigram])\n",
    "                left_term = (bigram_count - discount)/preceding_word_count\n",
    "            right_term = 0\n",
    "            if bigram[1] in self.stats.unigram_to_count:\n",
    "                current_word_count = self.stats.unigram_to_count[bigram[1]]\n",
    "                num_bigram_preceding_word = 0\n",
    "                for c_bigram in self.stats.bigram_to_count.keys():\n",
    "                    if c_bigram[0] == bigram[0] :\n",
    "                        num_bigram_preceding_word += 1\n",
    "                normalization_param = (discount * num_bigram_preceding_word)/ preceding_word_count \n",
    "                p_unigram = current_word_count/self.unigram_freq\n",
    "                right_term = normalization_param * p_unigram\n",
    "            return left_term + right_term\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prop_abs_dist(ngram_stats, preceding_unigram, d = 0.75):\n",
    "    \"\"\" Compute the distribution p(y | x) of all y given preceding_unigram\n",
    "\n",
    "        Args:\n",
    "            preceding_unigram (string) : the preceding unigram.\n",
    "            d (float) : the discounter factor for the linear interpolation.\n",
    "    \"\"\"\n",
    "    absDist = AbsDist(ngram_stats)\n",
    "    c_unigram_to_prob = dict()\n",
    "    for c_unigram in ngram_stats.unigram_to_count.keys():\n",
    "        if not c_unigram in c_unigram_to_prob:\n",
    "            c_unigram_to_prob[c_unigram] = absDist.compute_prop((preceding_unigram, c_unigram), d)\n",
    "  \n",
    "    sorted_prob = sorted(c_unigram_to_prob.items(), key=operator.itemgetter(1))\n",
    "    return sorted_prob\n",
    "\n",
    "print(compute_prop_abs_dist(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prop_KN(ngram_stats, preceding_word):\n",
    "    # Implement Kneser Ney Smoothing here.\n",
    "    # Hint: try to reuse the above code as much as possible.\n",
    "    pass\n",
    "\n",
    "\n",
    "print(compute_prop_KN(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the differences regarding prediction results here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 [2 pts]. Transition-based Dependency Parsing.\n",
    "\n",
    "We have learned in the lecture that Nivre's parsing algorithm has four parsing actions (**Left-Arc**, **Right-Arc**, **Reduce**, **Shift**). There are also several alternative transition-based algorithms for dependency parsing. Please learn the **Arc-Standard** algorithm from http://stp.lingfil.uu.se/~sara/kurser/5LN455-2014/lectures/5LN455-F8.pdf, and apply this algorithm to the following sentence. Use the same grammar as given below for parsing.\n",
    "\n",
    "_{Noun->Adj, ROOT->Verb, Noun->Det, Verb->Prep, Verb->Noun, figure->on, on->screen}_\n",
    "\n",
    "Check if you are able to find a priority queue that generates the same parsing tree as in the slides of dependency parsing. Write down all intermediate transitions. \n",
    "\n",
    "__Red figures on the screen indicated falling stocks__\n",
    "\n",
    "Note: \n",
    "\n",
    "-  The intermediate transitions should be written as:\n",
    "\n",
    "...\n",
    "\n",
    "_LA\t<ROOT, figures on the screen indicated falling stocks,  {(figures, Red)}>_\n",
    "\n",
    "_S\t<figures ROOT, on the screen indicated falling stocks, {(figures, Red)}>_\n",
    "\n",
    "...\n",
    "\n",
    "-  In the priority queue, you may include pre-conditions before applying parsing actions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__SOLUTION__:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
