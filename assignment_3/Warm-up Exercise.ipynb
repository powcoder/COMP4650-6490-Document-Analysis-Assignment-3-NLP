{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "# Import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice of <a href='https://www.tensorflow.org/api_docs/python/tf/constant'>constants</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('constants'):\n",
    "    a = tf.constant([-1,1, 1, -1], shape = [2,2], name='a')\n",
    "    b = tf.constant([0,1], shape = [2,1], name = 'b')\n",
    "    c = tf.matmul(a, b, name = 'multiply')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why you cannot get the expected [1, -1]?\n",
    "# What if we run the matrix multiplication with tf.Session?\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the computation graph\n",
    "logs_dir = './computation_graphs'\n",
    "with tf.Session() as sess:\n",
    "    log_writer = tf.summary.FileWriter(logs_dir, sess.graph)\n",
    "    sess.run(c)\n",
    "log_writer.close()\n",
    "# run tensorboard --logdir=\"./computation_graphs\" to visualize the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A1: Compute np.matmul(np.array([[1,2],[3,4],[5,6]]), np.array([[1],[2]]) ) + np.array([1,0,1]) with Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice of <a href='https://www.tensorflow.org/api_docs/python/tf/Variable'>variables</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.Variable(tf.random_normal([3,2], mean=0, stddev=1), name = 'M')\n",
    "m_times_two = M.assign(M * 2)\n",
    "init_m = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_m)\n",
    "    print('M = %s\\n' % M.eval())\n",
    "    sess.run(m_times_two)\n",
    "    print('M * 2 = %s\\n' %  M.eval())\n",
    "    sess.run(m_times_two)\n",
    "    print('M * 4 = %s\\n' % M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a place holder of type float 32-bit.\n",
    "v = tf.placeholder(tf.float32, shape = [2, 1], name = 'v')\n",
    "m_times_v = tf.matmul(M, v, name = 'M_v')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_m)\n",
    "    print('M = %s\\n' % M.eval())\n",
    "    print('M * [2,1] = %s \\n' % sess.run(m_times_v, {v: [[2],[1]]}))\n",
    "    print('M * [1,2] = %s \\n' % sess.run(m_times_v, {v: [[1],[2]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A2: Randomly generate a matrix W and a vector b, compute W * x + b, where x are column vectors from a set of randomly generated vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of word embedding look up.\n",
    "x = tf.placeholder(tf.int32, shape = [None], name = 'v')\n",
    "embeddings = tf.Variable(tf.random_uniform([10, 5], -1.0, 1.0), name = 'embed')\n",
    "embed_seq = tf.nn.embedding_lookup(embeddings, x)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('embed = %s\\n' % embeddings.eval())\n",
    "    print('sequence 1 \\n')\n",
    "    print(sess.run(tf.reshape(tf.reduce_sum(embed_seq, 0), [5,1]), {x: np.array([1,2,3])}))\n",
    "    print('sequence 2 \\n')\n",
    "    print(sess.run(embed_seq, {x: np.array([1,3])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and restore computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_x = tf.placeholder(tf.int32, shape = [None, 6], name = 'v')\n",
    "embeddings = tf.get_variable('embed', initializer = tf.random_uniform([10, 6], -1.0, 1.0))\n",
    "embed_seq = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "\n",
    "mean_vec = tf.reduce_mean(embed_seq, 1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "file_path ='./save_restore_test'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(mean_vec, {input_x: np.random.randint(0,10,[4, 6])}))\n",
    "    print(sess.run(mean_vec, {input_x: np.random.randint(0,10,[1, 6])}))\n",
    "    saver.save(sess, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3: Replace input_x = tf.placeholder(tf.int32, shape = [None, 6], name = 'v') with input_x = tf.placeholder(tf.int32, shape = [4, 6], name = 'v'), see what happens. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, file_path)\n",
    "    print(sess.run([embeddings, mean_vec], {input_x: np.random.randint(0,9,[1, 6])}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "A4: Use the following code or the Gensim library to load pre-trained word embeddings, and compute the means of word sequences.\n",
    "\n",
    "-  https://www.tensorflow.org/tutorials/representation/word2vec\n",
    "-  http://projector.tensorflow.org/\n",
    "-  https://radimrehurek.com/gensim/models/word2vec.html\n",
    "-  https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GloVe(file_path):\n",
    "    \"\"\"\n",
    "        Load word embeddings in the format of GloVe.\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = dict()\n",
    "    embd = []\n",
    "    with open(file_path,'r') as file:\n",
    "        for line in file.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab[row[0]] = len(vocab)\n",
    "            embd.append(row[1:])\n",
    "        print('Loaded GloVe!')\n",
    "    embedding = np.asarray(embd, dtype=np.float32)\n",
    "    return vocab,embedding\n",
    "\n",
    "def load_Word2Vec(file_path):\n",
    "    \"\"\"\n",
    "        Load word embeddings in the format of Word2Vec.\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = dict()\n",
    "    embd = []\n",
    "    with open(file_path,'r') as file:\n",
    "        head_line = file.readline()\n",
    "        head_line_tokens = head_line.strip().split(' ')\n",
    "        num_words = int(head_line_tokens[0])\n",
    "        word_dim = int(head_line_tokens[1])\n",
    "        for line in file.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab[row[0]] = len(vocab)\n",
    "            embd.append(row[1:])\n",
    "            assert(len(embd) == word_dim)\n",
    "        print('Loaded word2vec!')\n",
    "    embedding = np.asarray(embd, dtype=np.float32)\n",
    "    return vocab,embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "More details can be found in the lecture note https://docs.google.com/document/d/1_ZqzBqFMV8YmdC2PmaTXOB9O1BZ14yLNTQd2_Bkff9Y/edit.\n",
    "\n",
    "Some tricks for RNN : http://ruder.io/deep-learning-nlp-best-practices/index.html#lstmtricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_inputs(batch_size, max_length, embedding_dim):\n",
    "    input_sens = np.random.rand(batch_size, max_length, embedding_dim)\n",
    "    sens_length = np.zeros(shape=[batch_size], dtype=np.int32)\n",
    "    sens_mask_list = []\n",
    "    if max_length > 1:\n",
    "        for i in range(0, batch_size):\n",
    "            sens_m_length = random.randint(1, max_length)\n",
    "            sens_length[i] = sens_m_length\n",
    "            for j in range(sens_m_length, max_length):\n",
    "                input_sens[i, j] = np.zeros(embedding_dim)\n",
    "    return (input_sens,np.array(sens_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 3\n",
    "max_length = 5\n",
    "embedding_dim = 2\n",
    "lstm_cell_size = 2\n",
    "\n",
    "input_sent, sent_length = generate_inputs(batch_size, max_length, embedding_dim)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define a computation graph for LSTM. It projects a sequence of word ids\n",
    "# into a sequence of hidden representations.\n",
    "with tf.name_scope('lstm_example'):\n",
    "    embed_seq_p = tf.placeholder(tf.float32, shape=[batch_size, max_length, embedding_dim])\n",
    "    sent_length_p = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_cell_size,forget_bias=1.0, state_is_tuple=True)\n",
    "    initial_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    hidden_seq, last_states = tf.nn.dynamic_rnn(lstm_cell, embed_seq_p, sequence_length=sent_length_p,\n",
    "    initial_state = initial_state,time_major=False, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    rep_values = sess.run(hidden_seq, feed_dict={embed_seq_p : input_sent, sent_length_p : sent_length})\n",
    "    print(rep_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 2\n",
    "max_length = 3\n",
    "embedding_dim = 2\n",
    "lstm_cell_size = 2\n",
    "\n",
    "input_sent, sent_length = generate_inputs(batch_size, max_length, embedding_dim)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('lstm_example'):\n",
    "    embed_seq_p = tf.placeholder(tf.float32, shape=[batch_size, max_length, embedding_dim])\n",
    "    sent_length_p = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    with tf.variable_scope('forward'):\n",
    "        fw_lstm = tf.contrib.rnn.BasicLSTMCell(lstm_cell_size,forget_bias=1.0, state_is_tuple=True)\n",
    "        fw_init_state = fw_lstm.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('backward'):\n",
    "        bw_lstm = tf.contrib.rnn.BasicLSTMCell(lstm_cell_size,forget_bias=1.0, state_is_tuple=True)\n",
    "        bw_init_state = bw_lstm.zero_state(batch_size, dtype=tf.float32)\n",
    "        \n",
    "    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        fw_lstm,\n",
    "        bw_lstm,\n",
    "        embed_seq_p,\n",
    "        sequence_length=sent_length_p,\n",
    "        initial_state_fw=fw_init_state,\n",
    "        initial_state_bw=bw_init_state, dtype=tf.float32, scope=\"BiLSTM\")\n",
    "    rep = tf.concat(outputs, 2)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    rep_values = sess.run(rep, feed_dict={embed_seq_p : input_sent, sent_length_p : sent_length})\n",
    "    print(rep_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A5: How do you apply uni-directional LSTM and a bidirectional LSTM for constructing sentence representations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A6: Tune the models developed for Q1 and Q2 based on the tips in https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d and http://ruder.io/deep-learning-nlp-best-practices/index.html#bestpractices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
